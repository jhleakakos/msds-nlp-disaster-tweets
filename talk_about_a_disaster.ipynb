{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:47.833655Z",
     "start_time": "2024-09-01T22:43:47.827984Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem Description\n",
    "\n",
    "This project uses deep learning to classify tweets based on if they are about real disasters. This is a Kaggle competition at https://www.kaggle.com/competitions/nlp-getting-started. The focus is on testing out recurrent neural networks (RNNs) to perform this classification.\n",
    "\n",
    "We will utilize natural language processing (NLP), a field of computer science that focuses on how computing technology can work with \"natural language,\" language that occurs naturally with humans. In this scenario, we are looking at a subset of tweets that are written in English and are illustrative of how people communicate in a natural manner with the English language. The \"natural\" aspect speaks to how people actually use the language rather than just focusing on the formal ways in which people are supposed to use the language. The RNNs we will test out require numbers as inputs, so our use of NLP will include converting written text into numeric representation as well as capturing relationships and context within the language. \n",
    "\n",
    "We will need the context abilities of RNNs since the use of disaster-related words on their own may not indicate an actual disaster. For example, someone may tweet \"This is a disaster,\" using the literal word \"disaster\" but referring metaphorically to a situation that is not a disaster that, for instance, a disaster relief group needs to respond to.\n",
    "\n",
    "After building the models, the competition asks us to submit files with target labels for the test set to evaluate and compare with other submissions based on F1 score."
   ],
   "id": "b41bfd10a8ed2693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Description & Initial Preprocessing",
   "id": "c3f4ac7c457214ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:47.935532Z",
     "start_time": "2024-09-01T22:43:47.888453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "df_sample_submission.head()"
   ],
   "id": "23cb71cf90a7b1a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:48.016931Z",
     "start_time": "2024-09-01T22:43:48.005958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'The training set has {df_train.shape[0]:,} rows and {df_train.shape[1]} columns')\n",
    "print(f'The test set has {df_test.shape[0]:,} rows and {df_test.shape[1]} columns')\n",
    "df_train.head()"
   ],
   "id": "d0db8e1657c987f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 7,613 rows and 5 columns\n",
      "The test set has 3,263 rows and 4 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset has three features we will be working with:\n",
    "- keyword: a keyword from the tweet and can be null\n",
    "- location: location from where the tweet was send and can be null\n",
    "- text: the body of a tweet\n",
    "\n",
    "First we check the overall size and shape of the data we are working with. The sample submission file in the end will need one row per `id` and `target` label, so nothing fancy. We also want to see if there are duplicates in the training set. The initial thought is that we will want to remove any rows that are duplicated across all columns except for `id` so that the modeling is not favoring identical rows that show up more than once.\n",
    "\n",
    "Note that the default for the `keep` param for `duplicated()` is first, meaning that the first instance of a duplicated row is flagged as valid and the remaining duplicated rows show up here. We want to keep one of each duplicated row, so the default for `keep` is good."
   ],
   "id": "a10a924b21dc12ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:48.159254Z",
     "start_time": "2024-09-01T22:43:48.112613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if any rows are duplicated across all features\n",
    "print(f'Train: there are {df_train[df_train.duplicated()].shape[0]} rows duplicated across all features, including the id feature')\n",
    "print(f'Test:  there are {df_test[df_test.duplicated()].shape[0]} rows duplicated across all features, including the id feature')\n",
    "print()\n",
    "\n",
    "# Check if any ids are duplicated\n",
    "print(f'Train: there are {df_train[df_train.duplicated(subset='id')].shape[0]} duplicated ids')\n",
    "print(f'Test:  there are {df_test[df_test.duplicated(subset='id')].shape[0]} duplicated ids')\n",
    "print()\n",
    "\n",
    "# Check if any rows are duplicated just in the text feature\n",
    "print(f'Train: there are {df_train[df_train.duplicated(subset='text')].shape[0]} rows duplicated across the text feature')\n",
    "print(f'Test:  there are {df_test[df_test.duplicated(subset='text')].shape[0]} rows duplicated across the text feature')\n",
    "print()\n",
    "\n",
    "# Check if any rows are duplicated across the keyword, location, and text features. \n",
    "# This is the duplication we are looking to address.\n",
    "print(f'Train: there are '\n",
    "    f'{df_train[df_train.duplicated(\n",
    "        subset=[\n",
    "            'keyword'\n",
    "            , 'location'\n",
    "            , 'text'\n",
    "        ]\n",
    "        # , keep=False # Returns 97 rows in total when this is uncommented\n",
    "    )].shape[0]} '\n",
    "  f'rows duplicated across the keyword, location, text features'\n",
    ")\n",
    "print(f'Test:  there are '\n",
    "    f'{df_test[df_test.duplicated(\n",
    "        subset=[\n",
    "            'keyword'\n",
    "            , 'location'\n",
    "            , 'text'\n",
    "        ]\n",
    "        # , keep=False # Returns 97 rows in total when this is uncommented\n",
    "    )].shape[0]} '\n",
    "      f'rows duplicated across the keyword, location, text features'\n",
    ")\n",
    "print()\n",
    "\n",
    "# Drop rows that are duplicated across the keyword, location, and text features\n",
    "df_train.drop_duplicates(\n",
    "    subset=[\n",
    "        'keyword'\n",
    "        , 'location'\n",
    "        , 'text'\n",
    "    ], inplace=True\n",
    ")\n",
    "df_test.drop_duplicates(\n",
    "    subset=[\n",
    "        'keyword'\n",
    "        , 'location'\n",
    "        , 'text'\n",
    "    ], inplace=True\n",
    ")\n",
    "\n",
    "print(f'Train: we end up with {df_train.shape[0]:,} rows remaining after dropping duplicates')\n",
    "print(f'Test:  we end up with {df_test.shape[0]:,} rows remaining after dropping duplicates')"
   ],
   "id": "e926ae2a7f33e19b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: there are 0 rows duplicated across all features, including the id feature\n",
      "Test:  there are 0 rows duplicated across all features, including the id feature\n",
      "\n",
      "Train: there are 0 duplicated ids\n",
      "Test:  there are 0 duplicated ids\n",
      "\n",
      "Train: there are 110 rows duplicated across the text feature\n",
      "Test:  there are 20 rows duplicated across the text feature\n",
      "\n",
      "Train: there are 61 rows duplicated across the keyword, location, text features\n",
      "Test:  there are 11 rows duplicated across the keyword, location, text features\n",
      "\n",
      "Train: we end up with 7,552 rows remaining after dropping duplicates\n",
      "Test:  we end up with 3,252 rows remaining after dropping duplicates\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We see 61 rows that are duplicated across all features except the primary key \"id\" feature. This is 0.8% out of the 7,613 total rows, so we can drop them without losing too much data.\n",
    "\n",
    "Next we need to clean up bad characters. We see some characters that do not look like they were encoded correctly in the original CSV. To simplify preprocessing at this stage, we are going to drop any rows that have non-ASCII characters. This is too broad of a net to cast for characters to address, but it is an okay starting place. In future iterations, this is something to return to with a more complicated strategy for cleaning up these characters, in particular finding a way to determine if there are valid non-ASCII characters in terms of meaning that we should keep for modeling."
   ],
   "id": "436e96842a865dad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:48.271596Z",
     "start_time": "2024-09-01T22:43:48.260548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example row with both bad characters and valid-but-not-ASCII characters.\n",
    "df_train[df_train['id'] == 3373]"
   ],
   "id": "c5f21402776eb5c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        id     keyword location  \\\n",
       "2345  3373  demolition      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "2345  General News Û¢åÊ'Demolition of houses on wat...       0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>3373</td>\n",
       "      <td>demolition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General News Û¢åÊ'Demolition of houses on wat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:43:50.576485Z",
     "start_time": "2024-09-01T22:43:50.497753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for non-ASCII chars in keyword feature -- note that we return 0 for this\n",
    "print(f'Train: rows where keyword feature has non-ASCII characters: {df_train[df_train['keyword'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}')\n",
    "print(f'Test:  rows where keyword feature has non-ASCII characters: {df_test[df_test['keyword'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}')\n",
    "print()\n",
    "\n",
    "# Check for non-ASCII chars in location feature\n",
    "print(f'Train: rows where location feature has non-ASCII characters: {df_train[df_train['location'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}')\n",
    "print(f'Test:  rows where location feature has non-ASCII characters: {df_test[df_test['location'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}')\n",
    "print()\n",
    "\n",
    "# Check for non-ASCII chars in text feature\n",
    "print(f'Train: rows where text feature has non-ASCII characters: {df_train[df_train['text'].str.contains(r'[^\\x00-\\x7F]')].shape[0]}')\n",
    "print(f'Test:  rows where text feature has non-ASCII characters: {df_test[df_test['text'].str.contains(r'[^\\x00-\\x7F]')].shape[0]}')\n",
    "print()\n",
    "\n",
    "# Check for non-ASCII chars in location or feature\n",
    "print(f'Train: rows where location or text feature has non-ASCII characters: '\n",
    "    f'{df_train[df_train['text'].str.contains(r'[^\\x00-\\x7F]') | df_train['location'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}'\n",
    ")\n",
    "print(f'Test:  rows where location or text feature has non-ASCII characters: '\n",
    "      f'{df_test[df_test['text'].str.contains(r'[^\\x00-\\x7F]') | df_test['location'].str.contains(r'[^\\x00-\\x7F]', na=False)].shape[0]}'\n",
    ")\n",
    "print()\n",
    "\n",
    "# Drop rows that have non-ASCII characters in location or text features\n",
    "df_train.drop(\n",
    "    df_train[df_train['text'].str.contains(r'[^\\x00-\\x7F]') | df_train['location'].str.contains(r'[^\\x00-\\x7F]')].index\n",
    "    , inplace=True\n",
    ")\n",
    "df_test.drop(\n",
    "    df_test[df_test['text'].str.contains(r'[^\\x00-\\x7F]') | df_test['location'].str.contains(r'[^\\x00-\\x7F]')].index\n",
    "    , inplace=True\n",
    ")\n",
    "\n",
    "print(f'Train: we end up with {df_train.shape[0]:,} rows remaining after dropping rows with non-ASCII characters in location or text feature')\n",
    "print(f'Test:  we end up with {df_test.shape[0]:,} rows remaining after dropping rows with non-ASCII characters in location or text feature')\n",
    "print()\n"
   ],
   "id": "e3662f8a7ce1fbd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: rows where keyword feature has non-ASCII characters: 0\n",
      "Test:  rows where keyword feature has non-ASCII characters: 0\n",
      "\n",
      "Train: rows where location feature has non-ASCII characters: 105\n",
      "Test:  rows where location feature has non-ASCII characters: 35\n",
      "\n",
      "Train: rows where text feature has non-ASCII characters: 682\n",
      "Test:  rows where text feature has non-ASCII characters: 328\n",
      "\n",
      "Train: rows where location or text feature has non-ASCII characters: 779\n",
      "Test:  rows where location or text feature has non-ASCII characters: 359\n",
      "\n",
      "Train: we end up with 6,773 rows remaining after dropping rows with non-ASCII characters in location or text feature\n",
      "Test:  we end up with 2,893 rows remaining after dropping rows with non-ASCII characters in location or text feature\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The final cleanup step before jumping into EDA and further preprocessing is to handle missing values since the models in a bit do not play nice with nulls.\n",
    "\n",
    "As we see in the next code block, there are 58 rows with null keyword values, and there are 2,278 rows with null locations. 58 is probably small enough for us to drop at this point, but 2,278 is too many to lose. Unfortunately, the missing keywords do not overlap with missing locations.\n",
    "\n",
    "We will drop the rows with NAs for the keyword feature, and we will manually scan through the rows with missing values for the location feature to see if any patterns pop out. If we are able to find an informed strategy, we might be able to leverage something smarter for imputing values."
   ],
   "id": "401178e666114bb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:46:46.632615Z",
     "start_time": "2024-09-01T22:46:46.613358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('train:')\n",
    "print(df_train.isna().sum())\n",
    "print()\n",
    "print('test:')\n",
    "print(df_test.isna().sum())\n",
    "print()\n",
    "df_train.dropna(subset='keyword').isna().sum()\n",
    "df_test.dropna(subset='keyword').isna().sum()\n",
    "\n",
    "df_train.dropna(\n",
    "    subset='keyword'\n",
    "    , inplace=True\n",
    ")\n",
    "df_test.dropna(\n",
    "    subset='keyword'\n",
    "    , inplace=True\n",
    ")"
   ],
   "id": "5d00bb2fc38429b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "id             0\n",
      "keyword       58\n",
      "location    2278\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "test:\n",
      "id            0\n",
      "keyword      24\n",
      "location    993\n",
      "text          0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:31:12.357300Z",
     "start_time": "2024-09-01T22:31:12.341086Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[df_train.isna().any(axis=1)].sample(500)",
   "id": "1344d5b9e6347c31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id       keyword location  \\\n",
       "6778   9710       tragedy      NaN   \n",
       "3098   4448  electrocuted      NaN   \n",
       "5661   8074        rescue      NaN   \n",
       "2663   3825      detonate      NaN   \n",
       "6931   9941       trouble      NaN   \n",
       "...     ...           ...      ...   \n",
       "7134  10218       volcano      NaN   \n",
       "3267   4692      engulfed      NaN   \n",
       "4960   7070      meltdown      NaN   \n",
       "1893   2724       crushed      NaN   \n",
       "3082   4424   electrocute      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "6778  Maaaaan I love Love Without Tragedy by @rihann...       0  \n",
       "3098  When I was cooking earlier I got electrocuted ...       0  \n",
       "5661  Sammy and todd always to the rescue may not be...       0  \n",
       "2663                   @OpTic_Scumper Yo why u so sexy?       0  \n",
       "6931  The worst  voice I can ever hear is the 'Nikki...       0  \n",
       "...                                                 ...     ...  \n",
       "7134  http://t.co/Ns1AgGFNxz #shoes Asics GT-II Supe...       0  \n",
       "3267  Men escape car engulfed in flames in Parley's ...       1  \n",
       "4960      Why must I have a meltdown every few days? ??       0  \n",
       "1893            Wow! He crushed that! #EDWING #BlueJays       0  \n",
       "3082  Why does my phone electrocute me when it's cha...       0  \n",
       "\n",
       "[500 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>9710</td>\n",
       "      <td>tragedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maaaaan I love Love Without Tragedy by @rihann...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>4448</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When I was cooking earlier I got electrocuted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>8074</td>\n",
       "      <td>rescue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sammy and todd always to the rescue may not be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>3825</td>\n",
       "      <td>detonate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@OpTic_Scumper Yo why u so sexy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6931</th>\n",
       "      <td>9941</td>\n",
       "      <td>trouble</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The worst  voice I can ever hear is the 'Nikki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>10218</td>\n",
       "      <td>volcano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/Ns1AgGFNxz #shoes Asics GT-II Supe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>4692</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Men escape car engulfed in flames in Parley's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>7070</td>\n",
       "      <td>meltdown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Why must I have a meltdown every few days? ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>2724</td>\n",
       "      <td>crushed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wow! He crushed that! #EDWING #BlueJays</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>4424</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Why does my phone electrocute me when it's cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unfortunately, there does not appear to be a pattern in how locations are missing. I want to keep location in the modeling, so we will impute \"nope\" for rows that are missing location. We will also add a boolean flag indicating if we imputed location so the model can have that as an extra input.",
   "id": "e7804c5de52e584d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T22:47:33.940830Z",
     "start_time": "2024-09-01T22:47:33.933187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note that this cell is not idempotent\n",
    "df_train['is_location_imputed'] = df_train['location'].isna().astype(int)\n",
    "df_train['location_imputed'] = df_train['location'].fillna('nope')\n",
    "\n",
    "df_test['is_location_imputed'] = df_test['location'].isna().astype(int)\n",
    "df_test['location_imputed'] = df_test['location'].fillna('nope')"
   ],
   "id": "9b961a8a95c9e2e5",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exploratory Data Analysis (EDA)",
   "id": "772ff46cd0e7c0f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T16:06:45.915375Z",
     "start_time": "2024-09-01T15:20:02.374010Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords: 222\n",
      "Number of unique locations: 3342\n"
     ]
    }
   ],
   "execution_count": 44,
   "source": [
    "print(f'Number of unique keywords: {df_train['keyword'].nunique(dropna=False)}')\n",
    "print(f'Number of unique locations: {df_train['location'].nunique(dropna=False)}')"
   ],
   "id": "f999432af7238767"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T16:06:45.934599Z",
     "start_time": "2024-09-01T15:20:04.023627Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "USA                    104\n",
       "New York                71\n",
       "United States           50\n",
       "London                  45\n",
       "Canada                  29\n",
       "                      ... \n",
       "MontrÌ©al, QuÌ©bec       1\n",
       "Montreal                 1\n",
       "ÌÏT: 6.4682,3.18287      1\n",
       "Live4Heed??              1\n",
       "Lincoln                  1\n",
       "Name: count, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48,
   "source": "df_train['location'].value_counts()",
   "id": "c9c8157282b6cfbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T16:06:45.936282Z",
     "start_time": "2024-09-01T15:20:04.852577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweet_num_chars = df_train['text'].str.len()\n",
    "tweet_num_words = df_train['text'].str.split().str.len()\n",
    "print('Character counts:')\n",
    "print(tweet_num_chars.describe())\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(tweet_num_words.describe())"
   ],
   "id": "ef8ea6a019a7faa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character counts:\n",
      "count    7613.000000\n",
      "mean      101.037436\n",
      "std        33.781325\n",
      "min         7.000000\n",
      "25%        78.000000\n",
      "50%       107.000000\n",
      "75%       133.000000\n",
      "max       157.000000\n",
      "Name: text, dtype: float64\n",
      "\n",
      "Word counts:\n",
      "count    7613.000000\n",
      "mean       14.903586\n",
      "std         5.732604\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        15.000000\n",
      "75%        19.000000\n",
      "max        31.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4866b90b9532900e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
